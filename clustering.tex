%!TEX root = root.tex

Partitionner un corpus vis-à-vis de l'égalité entre empreintes permet
déjà de regrouper des programmes syntaxiquement distincts mais qui ont
des structures calculatoires très proches, sinon égales. Nous
souhaitons aller plus loin en regroupant des programmes qui se
ressemblent même si leurs structures respectives diffèrent
plus significativement. Considérons par exemple:

\begin{ocaml}
let id1 x = print_endline "debug"; x and id2 x = x
\end{ocaml}
\noindent \iocaml{id1} et \iocaml{id2} n'ont pas le même glyphe mais
on peut difficilement ignorer leur ressemblance!

Comme nous l'avons expliqué précédemment, nous utilisons un algorithme
de partitionnement hiérarchique ascendant qui a besoin d'une notion
de dissimilarité pour fonctionner.

\begin{defn}
Soient $X$ et $Y$ deux empreintes. La \textit{dissimilarité} $d(X, Y)$ entre~$X$ et~$Y$
est une valeur de $\mathbb{N} \cup \{ \infty \}$ définie comme suit:
\[
d (X,Y) = \sum\limits_{g \in X \Delta Y} w(g) \text{ ou }
	  \infty \text{ si $X \cap Y = \emptyset$} \\
\]
\noindent où $X \Delta Y$ est la différence symétrique entre deux multi-ensembles~$X$ et~$Y$.
\end{defn}

Cette définition n'est pas très standard car elle sépare de façon très
brutale les programmes qui ne partagent aucun sous-terme. Ce choix
garantit que deux programmes sans rapport ne pourront jamais
apparaître dans la même classe. Ainsi, le résultat final sera composé
d'une union disjointe de classes infiniment distantes les unes des
autres. Nous pensons que cette propriété améliore la lisibilité des
résultats.

Notez que cette notion de dissimilarité n'est pas une distance. En
effet, elle ne vérifie pas l'inégalité triangulaire. Par contre, c'est
une fonction de séparation ($\forall X,Y,\ d(X,Y) = 0 \iff X = Y$) et
symétrique ($\forall X,Y,\ d(X,Y) = d(Y,X)$). Cette propriété est
suffisante pour appliquer l'algorithme de partitionnement.

L'algorithme procède par itération sur une liste de classes
d'empreintes. On suppose que l'on sait fusionner deux classes
d'empreintes et que l'on garde trace de ces fusions pour pouvoir
produire un dendrogramme par classe.

\begin{ocaml}
(* Types pour les classes d'empreintes et les partionnements. *)
type cluster and clustering
(* L'opération de fusion. *)
val merge : cluster -> cluster -> clustering -> clustering
(* Un partitionnement initial avec une empreinte par classe. *)
val initial : fingerprint list -> clustering
(* Le nombre de classes d'un partitionnement. *)
val size : clustering -> int
\end{ocaml}

Il faut étendre la notion de dissimilarité aux paires de classes.

\begin{defn}
La dissimilarité~$d(\alpha, \beta)$ entre deux classes
d'empreintes~$\alpha$ et~$\beta$ est:
\begin{align*}
d(\alpha,\beta) &= \max\limits_{X \in \alpha, Y \in \beta} d(X,Y)
\end{align*}
\end{defn}

Cette définition de la dissimilarité inter-classes est classique et
donne lieu à un partitionnement dit ``à liaison complète'' (\emph{complete
  linkage clustering})~\cite{DBLP:books/crc/aggarwal13/ReddyV13}.

Enfin, on suppose l'existence d'une fonction capable de déterminer
les deux classes les plus proches dans le partitionnement courant:

\begin{ocaml}
type dissimilarity = Regular of int | Infinity
(* Renvoie les deux clusters les plus proches selon d, ainsi que leur distance *)
val get_closest_with_d : clustering -> (dissimilarity * (cluster * cluster))
\end{ocaml}

L'algorithme de partitionnement hiérarchique est donné par le programme {\OCaml} suivant:

\begin{ocaml}
(* Renvoie une liste de classes deux à deux infiniment dissimilaires. *)
let rec make_clustering xs =
  if size xs <= 1 then xs else match get_closest_with_d xs with
   | (Infinity, _) -> xs
   | (Regular p, (u, v)) -> make_clustering (merge u v xs)
let clustering fs = make_clustering @@ initial fs
\end{ocaml}

Cette formulation de l'algorithme est malheureusement trop naïve pour
être implémentée directement. En effet, en pire cas, la complexité de
cet algorithme est cubique en fonction de la longueur de la
liste~\iocaml{fs}, c'est-à-dire du nombre de définitions du
corpus et dans nos cas d'usage, ce nombre de l'ordre de~$10^6$.

Heureusement, il est possible d'en fournir une version optimisée
(restant de complexité cubique néanmoins). Cette optimisation
s'appuie sur les quatre étapes suivants :

\begin{enumerate}
\item
  On commence par séparer les empreintes en deux ensembles : celles
  qui sont infiniment dissimilaires des autres et celles qui ne le
  sont pas. Cette phase permet d'éliminer les définitions qui ne
  partagent absolument rien avec les autres, et sont donc exclues de
  toute forme de redondance. Cette passe a un coût quadratique en le
  nombre~$D$ d'empreintes.
\item
  Les dissimilarités inter-empreintes peuvent être précalculées une
  fois pour toute pour un coût quadratique en le nombre~$D$
  d'empreintes.  Ce précalcul, qui est effectué en parallèle sur
  plusieurs c{\oe}urs, permet de répondre en temps logarithmique
  \yrg{Pourquoi pas constant en copiant tout dans une matrice?}
  la valeur de $d(X, Y)$.
\item
  On calcule ensuite une surapproximation du partionnement : à l'aide
  d'une structure de type \textit{Union-Find}, on regroupe les
  empreintes qui ont une interdissimilarité finie. On sépare ainsi
  les empreintes qui ne pourront jamais être dans la même classe car
  elles sont infiniment dissimilaires deux-à-deux. Ce calcul a un coût
  en $O(D^2 \cdot log D \cdot \alpha(D))$. \yrg{Ici, on pourrait
    enlever le log si ma remarque précédente a du sens.}
\item
  Pour finir, l'algorithme de partitionnement naïf est exécuté en
  parallèle sur toutes les classes du partionnement sur-approximé.
  Cette étape a un coût cubique en la taille de chaque partition, ce
  qui réduit significativement le temps d'exécution de l'algorithme
  car, sur nos exemples, la cardinalité des classes ne dépasse
  jamais~$10^4$.

\end{enumerate}
